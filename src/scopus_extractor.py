"""
Extracteur ArXiv MASSIF - Version corrigÃ©e avec encodage des requÃªtes
OptimisÃ© pour rÃ©cupÃ©rer 100-500 articles en une fois
"""

import feedparser
import json
import pandas as pd
from datetime import datetime
import time
import os
from pathlib import Path
import urllib.parse
import requests


class MassiveArxivExtractor:
    def __init__(self):
        self.base_url = "http://export.arxiv.org/api/query"
        self.headers = {
            'User-Agent': 'MyArxivBot/0.1 (contact: lhroubasmae2018@gmail.com)'
        }
        self.all_articles = []

        # CrÃ©ation des dossiers
        Path('data/raw').mkdir(parents=True, exist_ok=True)
        Path('logs').mkdir(exist_ok=True)

        print("ğŸš€ Extracteur ArXiv MASSIF initialisÃ©")

   
    def search_articles_batch(self, query, start=0, max_results=25):
        """
        Recherche un batch d'articles via ArXiv API
        """
        # Encodage partiel : ne pas encoder :, ", ()
        query_encoded = urllib.parse.quote(query, safe=':"()')

        url = f"{self.base_url}?search_query={query_encoded}&start={start}&max_results={max_results}"
        print(f"ğŸ“¡ RequÃªte API ArXiv - URL: {url}")

        # Utilisation de requests pour inclure User-Agent
        response = requests.get(url, headers=self.headers)
        if response.status_code != 200:
            print(f"âš ï¸ Erreur API ArXiv: {response.status_code}")
            return []

        feed = feedparser.parse(response.text)
        return feed.entries


    def extract_article_info(self, entry):
        """
        Extrait les informations d'un article ArXiv
        """
        try:
            return {
                'arxiv_id': entry.id.split('/')[-1],
                'title': entry.title,
                'authors': [author.name for author in entry.authors],
                'abstract': entry.summary,
                'published': entry.published,
                'pdf_url': next((l.href for l in entry.links if 'pdf' in l.href), None),
                'categories': [t['term'] for t in entry.tags] if hasattr(entry, 'tags') else []
            }
        except Exception as e:
            print(f"âš ï¸ Erreur extraction article: {e}")
            return None

    def massive_extraction(self, queries, max_articles_per_query=100):
        """
        Extraction massive avec plusieurs requÃªtes
        """
        print("ğŸ¯ DÃ‰BUT DE L'EXTRACTION MASSIVE")
        print("=" * 50)

        total_extracted = 0

        for i, query in enumerate(queries, 1):
            print(f"\nğŸ” REQUÃŠTE {i}/{len(queries)}: {query}")
            print("-" * 40)

            extracted_for_query = 0
            start = 0

            while extracted_for_query < max_articles_per_query:
                remaining = max_articles_per_query - extracted_for_query
                count = min(50, remaining)  # ArXiv max = 50 par requÃªte

                entries = self.search_articles_batch(query, start=start, max_results=count)

                if not entries:
                    print("â„¹ï¸ Pas d'articles dans ce batch")
                    break

                # Traitement des articles
                for entry in entries:
                    article_info = self.extract_article_info(entry)
                    if article_info and article_info['title']:
                        self.all_articles.append(article_info)
                        extracted_for_query += 1
                        total_extracted += 1

                print(f"  âœ… Batch traitÃ©: {len(entries)} articles (+{extracted_for_query} total)")

                time.sleep(3)  # Pause pour Ã©viter surcharge
                start += count

                if extracted_for_query >= max_articles_per_query:
                    break

            print(f"âœ… RequÃªte terminÃ©e: {extracted_for_query} articles extraits")

            if i < len(queries):
                print("â³ Pause de 2 secondes...")
                time.sleep(2)

        print(f"\nğŸ‰ EXTRACTION TERMINÃ‰E!")
        print(f"ğŸ“Š TOTAL: {total_extracted} articles extraits")

        return total_extracted

    def save_articles(self):
        """
        Sauvegarde tous les articles
        """
        if not self.all_articles:
            print("âŒ Aucun article Ã  sauvegarder")
            return

        # Suppression des doublons basÃ©e sur arxiv_id
        seen_ids = set()
        unique_articles = []

        for article in self.all_articles:
            if article['arxiv_id'] not in seen_ids:
                seen_ids.add(article['arxiv_id'])
                unique_articles.append(article)

        duplicates_removed = len(self.all_articles) - len(unique_articles)
        if duplicates_removed > 0:
            print(f"ğŸ—‘ï¸ {duplicates_removed} doublons supprimÃ©s")

        # Sauvegarde JSON
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f'data/raw/arxiv_articles_massive_{timestamp}.json'

        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(unique_articles, f, indent=2, ensure_ascii=False)

        # Sauvegarde CSV
        csv_filename = f'data/raw/arxiv_articles_massive_{timestamp}.csv'
        df = pd.DataFrame(unique_articles)
        df.to_csv(csv_filename, index=False, encoding='utf-8')

        print(f"ğŸ’¾ Articles sauvegardÃ©s:")
        print(f"  ğŸ“„ JSON: {json_filename}")
        print(f"  ğŸ“Š CSV: {csv_filename}")
        print(f"  ğŸ“ˆ Total unique: {len(unique_articles)} articles")

        return json_filename


def main():
    """
    Fonction principale pour l'extraction massive
    """
    print("ğŸ“ PROJET ARXIV CHATBOT - EXTRACTION MASSIVE")
    print("=" * 50)

    # RequÃªtes pour extraction massive (avec espaces possibles)
    queries = [
    'ti:"artificial intelligence" OR abs:"artificial intelligence"',
    'ti:"machine learning" OR abs:"machine learning"',
    'ti:"deep learning" OR abs:"deep learning"',
    'ti:"neural networks" OR abs:"neural networks"',
    'ti:"computer vision" OR abs:"computer vision"',
    'ti:"natural language processing" OR abs:"NLP"',
    'ti:"transformer" OR abs:"transformer"',
    'ti:"large language model" OR abs:"LLM"',
    'ti:"reinforcement learning" OR abs:"reinforcement learning"',
    'ti:"speech recognition" OR abs:"speech recognition"',
    ]   


    print(f"ğŸ¯ {len(queries)} requÃªtes prÃ©parÃ©es")
    print(f"ğŸ“Š Objectif: ~{len(queries) * 50} articles")

    # Extraction
    extractor = MassiveArxivExtractor()
    total_extracted = extractor.massive_extraction(queries, max_articles_per_query=200)

    if total_extracted > 0:
        json_file = extractor.save_articles()

        print(f"\nğŸ‰ EXTRACTION MASSIVE TERMINÃ‰E!")
        print(f"ğŸ“Š {total_extracted} articles extraits")
        print(f"ğŸ“ Fichier crÃ©Ã©: {json_file}")
        print(f"\nğŸš€ Prochaine Ã©tape: Relancez le nettoyage avec ces nouvelles donnÃ©es!")
        print(f"   python src/data_processor.py")
    else:
        print(f"\nâŒ Aucun article extrait")


if __name__ == "__main__":
    main()
